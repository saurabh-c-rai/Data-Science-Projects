{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/raisaurabh04/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "import string\n",
    "import gensim\n",
    "from gensim.models.lsimodel import LsiModel\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_train = '/Users/raisaurabh04/OneDrive/GreyAtom/Practice Dataset/domain_classification_haptik_train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_test = '/Users/raisaurabh04/OneDrive/GreyAtom/Practice Dataset/domain_classification_haptik_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>food</th>\n",
       "      <th>recharge</th>\n",
       "      <th>support</th>\n",
       "      <th>reminders</th>\n",
       "      <th>travel</th>\n",
       "      <th>nearby</th>\n",
       "      <th>movies</th>\n",
       "      <th>casual</th>\n",
       "      <th>other</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7am everyday</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chocolate cake</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>closed mortice and tenon joint door dimentions</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train eppo kelambum</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yesterday i have cancelled the flight ticket</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>T</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "      <td>F</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          message food recharge support  \\\n",
       "0                                    7am everyday    F        F       F   \n",
       "1                                  chocolate cake    T        F       F   \n",
       "2  closed mortice and tenon joint door dimentions    F        F       T   \n",
       "3                             train eppo kelambum    F        F       F   \n",
       "4    yesterday i have cancelled the flight ticket    F        F       F   \n",
       "\n",
       "  reminders travel nearby movies casual other  \n",
       "0         T      F      F      F      F     F  \n",
       "1         F      F      F      F      F     F  \n",
       "2         F      F      F      F      F     F  \n",
       "3         F      T      F      F      F     F  \n",
       "4         F      T      F      F      F     F  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Action\n",
    "- We need to do an informal reverse of 'one hot encoding'.\n",
    "\n",
    "- Define a function label_race() which takes an argument row as input. This function should check every row for a category that is marked as T and return the name of the category.\n",
    "\n",
    "- Create a new column category which contains the values obtained by applying the above written function to all the rows of the dataframe. (Hint: use df.apply())\n",
    "\n",
    "- Drop the columns of food, recharge, support, reminders, nearby, movies, casual, other and travel from the dataframe df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new column called category which has the column marked as true for that particular message. \n",
    "def label_race(row):\n",
    "    for col in ['message','food', 'recharge', 'support', 'reminders', 'travel', 'nearby', 'movies', 'casual', 'other']:\n",
    "        if row[col] == 'T':\n",
    "            return col\n",
    "\n",
    "#%%timeit\n",
    "df['category'] = df.apply(label_race, axis=1)\n",
    "\n",
    "# Dropping all other columns except the category column\n",
    "df.drop(columns=['food', 'recharge', 'support', 'reminders', 'travel', 'nearby', 'movies', 'casual', 'other'], \n",
    "        inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7am everyday</td>\n",
       "      <td>reminders</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chocolate cake</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>closed mortice and tenon joint door dimentions</td>\n",
       "      <td>support</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train eppo kelambum</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>yesterday i have cancelled the flight ticket</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          message   category\n",
       "0                                    7am everyday  reminders\n",
       "1                                  chocolate cake       food\n",
       "2  closed mortice and tenon joint door dimentions    support\n",
       "3                             train eppo kelambum     travel\n",
       "4    yesterday i have cancelled the flight ticket     travel"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "<h3>Data Processing</h3>\n",
    "\n",
    "As we have seen in the Text Analytics concepts we need to convert this textual data into vectors so that we can apply machine learning algorithms to them. In this task we will now employ a normal TF-IDF vectorizer to vectorize the message column and label encode the category column, essentially making it a classification problem.\n",
    "\n",
    "Instructions\n",
    "- Since working on large data(40000 rows) will be time consuming, we have sampled 'df' to contain 1000 values of each category(Code already given)\n",
    "\n",
    "- Create a variable all_text and save all the values of column message converted in lower case into it\n",
    "\n",
    "- Instantiate \"TfidfVectorizer()\" with argument stop_words=\"english\"\n",
    "\n",
    "- Fit the above instantiated Tfidf Vectorizer object on all_text\n",
    "\n",
    "- Transform all_text using the above fitted model, convert it to an array and save the transformed array to a variable `X'\n",
    "\n",
    "- Instatiate a LabelEncoder() object.\n",
    "\n",
    "- fit the above instantiated model on category column of dataframe df\n",
    "\n",
    "- transform the 'category' column using the above fitted model and save the same to variable 'y'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling only 1000 samples of each category\n",
    "df = df.groupby('category').apply(lambda x: x.sample(n=1000, random_state=0))\n",
    "\n",
    "# Converting all messages to lower case and storing it\n",
    "all_text = df['message'].apply(lambda x : x.lower())\n",
    "\n",
    "# Initialising TF-IDF object\n",
    "tf_idf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "# Vectorizing data\n",
    "X = tf_idf.fit_transform(all_text)\n",
    "\n",
    "# Initiating a label encoder object\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Transforming the data and storing it\n",
    "y = label_encoder.fit_transform(df['category'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Classification implementation</h3>\n",
    "\n",
    "In the previous tasks we have cleaned the data and converted the textual data into numbers in order to enable us to apply machine learning models. In this task we will apply Logistic Regression , Naive Bayes and Lienar SVM model onto the data.\n",
    "\n",
    "Instructions\n",
    "\n",
    "- Split the feature array X and target variable y into training and validation sets using train_test_split() method and save the variables in X_train,X_val,y_train and y_val. Pass the parameter test_size=0.3, random_state=42 for the same\n",
    "\n",
    "- Instantiate a LogisticRegression(random_state=0) model and save it to variable log_reg\n",
    "\n",
    "- Fit the log_reg model on X_train and y_train\n",
    "\n",
    "- Predict the values using the above fitted model for X_val and store the same in y_pred\n",
    "\n",
    "- Calculate the accuracy score, store the same in variable log_accuracy and print the same\n",
    "\n",
    "- Repeat the above steps for MultinomialNB() model instantiated as nb, save the accuracy score in nb_accuracy\n",
    "\n",
    "- Similarly, repeat the above steps for LinearSVC(random_state=0) model instantiated as lsvm, save the accuracy score in lsvm_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into train and test sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Implementing Logistic Regression model\n",
    "log_reg = LogisticRegression(random_state=0)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_val)\n",
    "\n",
    "log_accuracy = log_reg.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Multinomial NB model\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = nb.predict(X_val)\n",
    "\n",
    "nb_accuracy = nb.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementing Linear SVM model\n",
    "lsvm = LinearSVC(random_state=0)\n",
    "\n",
    "lsvm.fit(X_train, y_train)\n",
    "\n",
    "y_pred = lsvm.predict(X_val)\n",
    "lsvm_accuracy = lsvm.score(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7085185185185185, 0.7114814814814815, 0.7125925925925926)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_accuracy, nb_accuracy, lsvm_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Validation of test data</h3>\n",
    "Let's now see how well our models run on test set.\n",
    "\n",
    "Instructions\n",
    "- The dataframe 'df_test' containing the test set has already been loaded and cleaned for you(Code given).\n",
    "\n",
    "- Create a variable all_text and save all the values of column message of 'df_test' converted in lower case into it.\n",
    "\n",
    "- Transform all_text using the previously fitted model tfidf, convert it into array and save the transformed array to a variable `X_test'\n",
    "\n",
    "- Transform the category column of 'df_test' using the previously created le object and save the same to variable y_test\n",
    "\n",
    "- Predict the values using the previously fitted model of logistic regression log_reg for X_test and store the same in y_pred\n",
    "\n",
    "- Calculate the accuracy score, store the same in variable log_accuracy_2 and print the same\n",
    "\n",
    "- Repeat the last two steps for MultinomialNB() model fitted as nb, save the accuracy score in nb_accuracy_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the dataframe\n",
    "df_test = pd.read_csv(path_test)\n",
    "\n",
    "#Creating the new column category\n",
    "df_test[\"category\"] = df_test.apply (lambda row: label_race (row),axis=1)\n",
    "\n",
    "#Dropping the other columns\n",
    "drop= [\"food\", \"recharge\", \"support\", \"reminders\", \"nearby\", \"movies\", \"casual\", \"other\", \"travel\"]\n",
    "df_test=  df_test.drop(drop,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Nearest metro station</td>\n",
       "      <td>nearby</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Pick up n drop service trough cab</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I wants to buy a bick</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Show me pizza</td>\n",
       "      <td>food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is the cheapest package to andaman and ni...</td>\n",
       "      <td>travel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             message category\n",
       "0                              Nearest metro station   nearby\n",
       "1                  Pick up n drop service trough cab   travel\n",
       "2                              I wants to buy a bick    other\n",
       "3                                      Show me pizza     food\n",
       "4  What is the cheapest package to andaman and ni...   travel"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting the reviews into lower case\n",
    "all_text = df_test['message'].str.lower()\n",
    "\n",
    "# Transforming using the tfidf object - tfidf\n",
    "X_test = tf_idf.transform(all_text).toarray()\n",
    "\n",
    "# Transforming using label encoder object - le\n",
    "y_test = label_encoder.transform(df_test['category'])\n",
    "\n",
    "# Predicting using the logistic regression model - logreg\n",
    "y_pred = log_reg.predict(X_test)\n",
    "log_accuracy_2 = log_reg.score(X_test, y_test)\n",
    "\n",
    "# Predicting using the naive bayes model - nb\n",
    "y_pred = nb.predict(X_test)\n",
    "nb_accuracy_2 = nb.score(X_test, y_test)\n",
    "\n",
    "# Predicting using the linear svm model - lsvm\n",
    "y_pred = lsvm.predict(X_test)\n",
    "lsvm_accuracy_2 = lsvm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>LSA Modeling</h3>\n",
    "In this task, we will try to see how to use LSI on the entire dataset.\n",
    "\n",
    "Instructions\n",
    "- A cleaned list called doc_clean containing data of message column is already given.\n",
    "\n",
    "- Create a dictionary from 'doc_clean' using \"corpora.Dictionary()\" and store it in a variable called 'dictionary'\n",
    "\n",
    "- Create a word corpus of 'dictionary' using \"doc2bow()\" method and store the result in a variable called 'doc_term_matrix'\n",
    "\n",
    "- Initialise the LSI model \"LsiModel()\" with the parameters corpus=doc_term_matrix, num_topics=5, id2word=dictionary and store it in lsimodel.\n",
    "\n",
    "- Print(Use pprint()) the 5 topics using \"print_topics()\" method of lsimodel and have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a stopwords list\n",
    "stop = set(stopwords.words('english'))\n",
    "exclude = set(string.punctuation)\n",
    "lemma = WordNetLemmatizer()\n",
    "# Function to lemmatize and remove the stopwords\n",
    "def clean(doc):\n",
    "    stop_free = \" \".join([i for i in doc.lower().split() if i not in stop])\n",
    "    punc_free = \"\".join(ch for ch in stop_free if ch not in exclude)\n",
    "    normalized = \" \".join(lemma.lemmatize(word) for word in punc_free.split())\n",
    "    return normalized\n",
    "\n",
    "# Creating a list of documents from the complaints column\n",
    "list_of_docs = df[\"message\"].tolist()\n",
    "\n",
    "# Implementing the function for all the complaints of list_of_docs\n",
    "doc_clean = [clean(doc).split() for doc in list_of_docs]\n",
    "\n",
    "# Code starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.347*\"reminder\" + 0.267*\"like\" + 0.267*\"cancel\" + 0.266*\"would\" + '\n",
      "  '0.256*\"userid\" + 0.256*\"apiname\" + 0.256*\"offset\" + 0.256*\"exotel\" + '\n",
      "  '0.255*\"reminderlist\" + 0.255*\"taskname\"'),\n",
      " (1,\n",
      "  '0.831*\"want\" + 0.221*\"u\" + 0.187*\"know\" + 0.181*\"movie\" + 0.135*\"book\" + '\n",
      "  '0.128*\"ticket\" + 0.114*\"need\" + 0.107*\"hi\" + 0.095*\"please\" + '\n",
      "  '0.092*\"service\"'),\n",
      " (2,\n",
      "  '0.451*\"reminder\" + -0.328*\"call\" + -0.316*\"u\" + -0.233*\"wake\" + '\n",
      "  '0.205*\"water\" + -0.197*\"march\" + -0.192*\"wakeup\" + 0.185*\"every\" + '\n",
      "  '0.181*\"drink\" + 0.168*\"want\"'),\n",
      " (3,\n",
      "  '0.611*\"u\" + -0.419*\"want\" + 0.244*\"need\" + 0.238*\"reminder\" + '\n",
      "  '0.197*\"please\" + 0.143*\"movie\" + 0.117*\"service\" + -0.102*\"wake\" + '\n",
      "  '0.101*\"near\" + 0.101*\"help\"'),\n",
      " (4,\n",
      "  '-0.621*\"need\" + 0.510*\"u\" + -0.492*\"movie\" + -0.190*\"offer\" + 0.137*\"want\" '\n",
      "  '+ -0.115*\"ticket\" + -0.058*\"know\" + -0.051*\"today\" + 0.051*\"find\" + '\n",
      "  '-0.049*\"book\"')]\n"
     ]
    }
   ],
   "source": [
    "# Creating the dictionary id2word from our cleaned word list doc_clean\n",
    "dictionary = corpora.Dictionary(documents=doc_clean)\n",
    "\n",
    "# Creating the corpus\n",
    "doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "\n",
    "# Creating the LSi model\n",
    "lsimodel = LsiModel(corpus=doc_term_matrix, num_topics=5, id2word=dictionary)\n",
    "pprint(lsimodel.print_topics())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>LDA Modeling</h3>\n",
    "\n",
    "Next let's try to do topic modeling using LDA. We will first find the optimum no. of topics using coherence score and then create a model attaining to the optimum no. of topics.\n",
    "\n",
    "- A function called \"compute_coherence_values\" that computes and returns different coherence values along with the no. of topics is already defined\n",
    "\n",
    "- Call the function \"compute_coherence_values\" with the parameters passed dictionary=dictionary, corpus=doc_term_matrix, texts=doc_clean, start=1, limit=41, step=5. (We want to check the coherence score for topic num ranging from 1 to 40 with a gap of 5). Store the return of the function in variables 'topic_list' & 'coherence_value_list'\n",
    "\n",
    "- Find the no. of topics from 'topic_list' associated with the maximum coherence score of 'coherence_value_list'(Hint: They will have same index) and store that no. in a variable called 'opt_topic'.\n",
    "\n",
    "- Print 'opt_topic' to take a look at the optimum no. of topics.\n",
    "\n",
    "- Initialize LdaModel() with the parameters corpus=doc_term_matrix, num_topics=opt_topic, id2word = dictionary, iterations=10 , passes=30& random_state=0 and store it in 'lda_model'.\n",
    "\n",
    "- Print(use pprint() instead of print()) five of the topics using \"print_topics(5)\" method of lda_model and have a look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "# doc_term_matrix - Word matrix created in the last task\n",
    "# dictionary - Dictionary created in the last task\n",
    "\n",
    "# Function to calculate coherence values\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    topic_list : No. of topics chosen\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    topic_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(doc_term_matrix, random_state = 0, num_topics=num_topics, id2word = dictionary, iterations=10)\n",
    "        topic_list.append(num_topics)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return topic_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:504: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not i.flags.writeable or i.dtype not in (np.int32, np.int64):\n",
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/scipy/sparse/lil.py:506: FutureWarning: future versions will not create a writeable array from broadcast_array. Set the writable flag explicitly to avoid this warning.\n",
      "  if not j.flags.writeable or j.dtype not in (np.int32, np.int64):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36\n",
      "[(12,\n",
      "  '0.221*\"show\" + 0.123*\"center\" + 0.112*\"service\" + 0.069*\"discount\" + '\n",
      "  '0.028*\"cake\" + 0.026*\"message\" + 0.024*\"paytm\" + 0.014*\"surat\" + '\n",
      "  '0.012*\"whether\" + 0.011*\"previous\"'),\n",
      " (8,\n",
      "  '0.190*\"find\" + 0.136*\"give\" + 0.118*\"booking\" + 0.073*\"u\" + 0.033*\"nearest\" '\n",
      "  '+ 0.032*\"full\" + 0.026*\"okay\" + 0.021*\"take\" + 0.018*\"star\" + '\n",
      "  '0.015*\"receipt\"'),\n",
      " (14,\n",
      "  '0.174*\"detail\" + 0.048*\"received\" + 0.047*\"note\" + 0.042*\"mean\" + '\n",
      "  '0.034*\"ask\" + 0.026*\"branch\" + 0.025*\"people\" + 0.022*\"u\" + 0.018*\"parlour\" '\n",
      "  '+ 0.016*\"gud\"'),\n",
      " (28,\n",
      "  '0.351*\"please\" + 0.090*\"fare\" + 0.083*\"good\" + 0.044*\"suggest\" + '\n",
      "  '0.033*\"review\" + 0.033*\"provide\" + 0.031*\"u\" + 0.030*\"see\" + 0.029*\"office\" '\n",
      "  '+ 0.016*\"abt\"'),\n",
      " (4,\n",
      "  '0.226*\"number\" + 0.081*\"shop\" + 0.064*\"store\" + 0.060*\"hyderabad\" + '\n",
      "  '0.032*\"sunday\" + 0.025*\"great\" + 0.022*\"3rd\" + 0.020*\"u\" + 0.017*\"window\" + '\n",
      "  '0.015*\"door\"')]\n"
     ]
    }
   ],
   "source": [
    "topic_list, coherence_value_list = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=doc_clean, start=1, limit=41, step=5)\n",
    "\n",
    "opt_topic = topic_list[np.argmax(coherence_value_list)]\n",
    "\n",
    "print(opt_topic)\n",
    "\n",
    "topic_list, coherence_value_list\n",
    "\n",
    "lda_model = LdaModel(corpus=doc_term_matrix, num_topics=opt_topic, id2word=dictionary, iterations=10, passes=30, random_state=0)\n",
    "\n",
    "pprint(lda_model.print_topics(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
