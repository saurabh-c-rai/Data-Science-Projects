{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.svm import SVC\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'When my loan was switched over to Navient i was never told that i had a deliquint balance because with XXXX i did not. When going to purchase a vehicle i discovered my credit score had been dropped from the XXXX into the XXXX. I have been faithful at paying my student loan. I was told that Navient was the company i had delinquency with. I contacted Navient to resolve this issue you and kept being told to just contact the credit bureaus and expalin the situation and maybe they could help me. I was so angry that i just hurried and paid the balance off and then after tried to dispute the delinquency with the credit bureaus. I have had so much trouble bringing my credit score back up.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "array(['Mortgage', 'Student loan', 'Credit card or prepaid card',\n",
       "       'Credit card', 'Debt collection', 'Credit reporting',\n",
       "       'Credit reporting, credit repair services, or other personal consumer reports',\n",
       "       'Bank account or service', 'Consumer Loan', 'Money transfers',\n",
       "       'Vehicle loan or lease',\n",
       "       'Money transfer, virtual currency, or money service',\n",
       "       'Checking or savings account', 'Payday loan',\n",
       "       'Payday loan, title loan, or personal loan',\n",
       "       'Other financial service', 'Prepaid card'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_2 = '/Users/raisaurabh04/OneDrive/GreyAtom/Practice Dataset/full_complaint_nlp_path_2.csv'\n",
    "\n",
    "path = '/Users/raisaurabh04/OneDrive/GreyAtom/Practice Dataset/new_complaint_nlp_path.csv'\n",
    "\n",
    "full_data = pd.read_csv(path)\n",
    "\n",
    "data = full_data[['Consumer complaint narrative', 'Product']]\n",
    "\n",
    "data.rename(columns={'Product' : 'y', 'Consumer complaint narrative' : 'X'}, inplace=True)\n",
    "\n",
    "data['X'][1]\n",
    "\n",
    "data['y'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "data.dropna(inplace=True)\n",
    "\n",
    "first_complaint = data.iloc[0, 0]\n",
    "\n",
    "bag_of_words_1 = first_complaint.split()\n",
    "\n",
    "bag_of_words_2 = word_tokenize(first_complaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_1 in bag_of_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'my',\n",
       " 'loan',\n",
       " 'was',\n",
       " 'switched',\n",
       " 'over',\n",
       " 'to',\n",
       " 'Navient',\n",
       " 'i',\n",
       " 'was',\n",
       " 'never',\n",
       " 'told',\n",
       " 'that',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'deliquint',\n",
       " 'balance',\n",
       " 'because',\n",
       " 'with',\n",
       " 'XXXX',\n",
       " 'i',\n",
       " 'did',\n",
       " 'not.',\n",
       " 'When',\n",
       " 'going',\n",
       " 'to',\n",
       " 'purchase',\n",
       " 'a',\n",
       " 'vehicle',\n",
       " 'i',\n",
       " 'discovered',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'score',\n",
       " 'had',\n",
       " 'been',\n",
       " 'dropped',\n",
       " 'from',\n",
       " 'the',\n",
       " 'XXXX',\n",
       " 'into',\n",
       " 'the',\n",
       " 'XXXX.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'faithful',\n",
       " 'at',\n",
       " 'paying',\n",
       " 'my',\n",
       " 'student',\n",
       " 'loan.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'told',\n",
       " 'that',\n",
       " 'Navient',\n",
       " 'was',\n",
       " 'the',\n",
       " 'company',\n",
       " 'i',\n",
       " 'had',\n",
       " 'delinquency',\n",
       " 'with.',\n",
       " 'I',\n",
       " 'contacted',\n",
       " 'Navient',\n",
       " 'to',\n",
       " 'resolve',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'you',\n",
       " 'and',\n",
       " 'kept',\n",
       " 'being',\n",
       " 'told',\n",
       " 'to',\n",
       " 'just',\n",
       " 'contact',\n",
       " 'the',\n",
       " 'credit',\n",
       " 'bureaus',\n",
       " 'and',\n",
       " 'expalin',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'they',\n",
       " 'could',\n",
       " 'help',\n",
       " 'me.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'so',\n",
       " 'angry',\n",
       " 'that',\n",
       " 'i',\n",
       " 'just',\n",
       " 'hurried',\n",
       " 'and',\n",
       " 'paid',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'off',\n",
       " 'and',\n",
       " 'then',\n",
       " 'after',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'dispute',\n",
       " 'the',\n",
       " 'delinquency',\n",
       " 'with',\n",
       " 'the',\n",
       " 'credit',\n",
       " 'bureaus.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had',\n",
       " 'so',\n",
       " 'much',\n",
       " 'trouble',\n",
       " 'bringing',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'score',\n",
       " 'back',\n",
       " 'up.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['When',\n",
       " 'my',\n",
       " 'loan',\n",
       " 'was',\n",
       " 'switched',\n",
       " 'over',\n",
       " 'to',\n",
       " 'Navient',\n",
       " 'i',\n",
       " 'was',\n",
       " 'never',\n",
       " 'told',\n",
       " 'that',\n",
       " 'i',\n",
       " 'had',\n",
       " 'a',\n",
       " 'deliquint',\n",
       " 'balance',\n",
       " 'because',\n",
       " 'with',\n",
       " 'XXXX',\n",
       " 'i',\n",
       " 'did',\n",
       " 'not',\n",
       " '.',\n",
       " 'When',\n",
       " 'going',\n",
       " 'to',\n",
       " 'purchase',\n",
       " 'a',\n",
       " 'vehicle',\n",
       " 'i',\n",
       " 'discovered',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'score',\n",
       " 'had',\n",
       " 'been',\n",
       " 'dropped',\n",
       " 'from',\n",
       " 'the',\n",
       " 'XXXX',\n",
       " 'into',\n",
       " 'the',\n",
       " 'XXXX',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'been',\n",
       " 'faithful',\n",
       " 'at',\n",
       " 'paying',\n",
       " 'my',\n",
       " 'student',\n",
       " 'loan',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'told',\n",
       " 'that',\n",
       " 'Navient',\n",
       " 'was',\n",
       " 'the',\n",
       " 'company',\n",
       " 'i',\n",
       " 'had',\n",
       " 'delinquency',\n",
       " 'with',\n",
       " '.',\n",
       " 'I',\n",
       " 'contacted',\n",
       " 'Navient',\n",
       " 'to',\n",
       " 'resolve',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'you',\n",
       " 'and',\n",
       " 'kept',\n",
       " 'being',\n",
       " 'told',\n",
       " 'to',\n",
       " 'just',\n",
       " 'contact',\n",
       " 'the',\n",
       " 'credit',\n",
       " 'bureaus',\n",
       " 'and',\n",
       " 'expalin',\n",
       " 'the',\n",
       " 'situation',\n",
       " 'and',\n",
       " 'maybe',\n",
       " 'they',\n",
       " 'could',\n",
       " 'help',\n",
       " 'me',\n",
       " '.',\n",
       " 'I',\n",
       " 'was',\n",
       " 'so',\n",
       " 'angry',\n",
       " 'that',\n",
       " 'i',\n",
       " 'just',\n",
       " 'hurried',\n",
       " 'and',\n",
       " 'paid',\n",
       " 'the',\n",
       " 'balance',\n",
       " 'off',\n",
       " 'and',\n",
       " 'then',\n",
       " 'after',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'dispute',\n",
       " 'the',\n",
       " 'delinquency',\n",
       " 'with',\n",
       " 'the',\n",
       " 'credit',\n",
       " 'bureaus',\n",
       " '.',\n",
       " 'I',\n",
       " 'have',\n",
       " 'had',\n",
       " 'so',\n",
       " 'much',\n",
       " 'trouble',\n",
       " 'bringing',\n",
       " 'my',\n",
       " 'credit',\n",
       " 'score',\n",
       " 'back',\n",
       " 'up',\n",
       " '.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bag_of_words_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_sentences = sent_tokenize(first_complaint)\n",
    "\n",
    "first_complaint_lower = first_complaint.lower()\n",
    "\n",
    "bag_of_words_lower = word_tokenize(first_complaint_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = Counter(bag_of_words_lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>For X</h3>\n",
    "\n",
    "- Store the \"X\" column of dataframe 'data'in a new dataframe 'all_text'(i.e. instead of data[\"X\"] use data[[\"X\"]])\n",
    "\n",
    "- Convert the values of X column of all_text into lowercase using \"lower()\" method.\n",
    "\n",
    "- Initialise a \"CountVectorizer()\" object and store it in 'cv'\n",
    "\n",
    "- \"fit_transform()\" the 'cv' on all_text[X] column and store the result in 'vector'\n",
    "\n",
    "- Convert 'vector' into an array using \"toarray()\" method and store the result in a new variable 'X'\n",
    "\n",
    "<h3>For y</h3>\n",
    "\n",
    "- Store the \"y\" column of dataframe 'data'in a new dataframe 'labels'(i.e. instead of data[\"y\"] use data[[\"y\"]])\n",
    "\n",
    "- We need to label encode the values. Therefore initialise a \"LabelEncoder()\" object and store it in 'le'\n",
    "\n",
    "- fit_transform method the 'le' on \"labels[y]\" and store the results back in labels[y].\n",
    "\n",
    "  If we now, include all the 335 rows and vectorize them and label it as the X, and the corresponding y labels, we can train a classification algorithm on the same, and figure out the accuracy.\n",
    "\n",
    "<h3>Model building</h3>\n",
    "- Split 'X' and 'labels[\"y\"]' into X_train,X_test,y_train,y_test using train_test_split() function. Use test_size = 0.4 and random_state = 42\n",
    "\n",
    "- Initialise a logistic regression model with LogisticRegression() having random_state=42 and save it to a variable called 'log_reg'.\n",
    "\n",
    "- Fit the model on the training data 'X_train' and 'y_train' using the 'fit()' method.\n",
    "\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'acc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when my loan was switched over to navient i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i tried to sign up for a spending monitoring p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my mortgage is with bb &amp; t bank, recently i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the entire lending experience with citizens ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my credit score has gone down xxxx points in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X\n",
       "1   when my loan was switched over to navient i wa...\n",
       "2   i tried to sign up for a spending monitoring p...\n",
       "7   my mortgage is with bb & t bank, recently i ha...\n",
       "13  the entire lending experience with citizens ba...\n",
       "14  my credit score has gone down xxxx points in t..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = data[['X']]\n",
    "\n",
    "all_text = all_text.applymap(lambda x : x.lower())\n",
    "all_text.head()\n",
    "cv = CountVectorizer()\n",
    "\n",
    "vector = cv.fit_transform(all_text['X'])\n",
    "\n",
    "X = vector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "labels = data[['y']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels['y'] = le.fit_transform(labels['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((335, 4134), (335,))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, labels['y'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, labels[\"y\"], test_size=0.40, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "acc = log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Stopword handling</h3>\n",
    "\n",
    "- Initialise a \"CountVectorizer()\" object with parameter stop_words= \"english\" and store it in 'cv_stop'\n",
    "- Apply the \"fit_transform()\" method of 'cv' on X column and store the result in 'vector_stop'\n",
    "- Convert 'vector_stop' into an array using \"toarray()\" method and store the result in a new variable 'X_stop'\n",
    "<h3>Model building</h3>\n",
    "\n",
    "- Split 'X_stop' and 'labels[\"y\"]' into X_train,X_test,y_train,y_test using train_test_split() function. Use test_size = 0.4 and random_state = 42\n",
    "- Initialise a logistic regression model with LogisticRegression() having random_state=42 and save it to a variable called 'log_reg'.\n",
    "- Fit the model on the training data 'X_train' and 'y_train' using the 'fit()' method.\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'stop_acc'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_stop = CountVectorizer(stop_words='english')\n",
    "\n",
    "vector_stop = cv_stop.fit_transform(all_text['X'])\n",
    "\n",
    "X_stop = vector_stop.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_stop, labels['y'], test_size=0.4, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "stop_acc = log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>TF-IDF scoring</h3>\n",
    "\n",
    "- Initialise a \"TfidfVectorizer()\" object with parameter stop_words= \"english\" and store it in 'tfidf'\n",
    "- Apply the \"fit_transform()\" method of 'tfidf' on X column and store the result in 'vector_tfidf'\n",
    "- Convert 'vector_tfidf' into an array using \"toarray()\" method and store the result in a new variable 'X_tfidf'\n",
    "\n",
    "<h3>Model building</h3>\n",
    "\n",
    "- Split 'X_tfidf' and 'labels[\"y\"]' into X_train,X_test,y_train,y_test using train_test_split() function. Use test_size = 0.4 and random_state = 42\n",
    "- Initialise a logistic regression model with LogisticRegression() having random_state=42 and save it to a variable called 'log_reg'.\n",
    "- Fit the model on the training data 'X_train' and 'y_train' using the 'fit()' method.\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'tfidf_acc'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "vector_tfidf = tfidf.fit_transform(all_text['X'])\n",
    "\n",
    "X_tfidf = vector_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n",
       "                   multi_class='warn', n_jobs=None, penalty='l2',\n",
       "                   random_state=42, solver='warn', tol=0.0001, verbose=0,\n",
       "                   warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels['y'], test_size=0.4, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(random_state=42)\n",
    "\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "tfidf_acc = log_reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Data Loading</h3>\n",
    "\n",
    "- Load the dataset from 'path_2'(given) using the read_csv() method from pandas and store it in 'data'.\n",
    "- Subset the dataframe 'data' to only include \"Consumer complaint narrative\" and \"Product\" and store this dataframe subset back in 'data'\n",
    "- Rename the column \"Consumer complaint narrative\" to \"X\" and \"Product\" to \"y\" by assigning [\"X\",\"y\"] to data.columns\n",
    "- Drop nan values from data using \"dropna()\" and save it back to 'data'\n",
    "\n",
    "<h3>For X</h3>\n",
    "\n",
    "- Store the \"X\" column of dataframe 'data'in a new dataframe 'all_text'(i.e. instead of data[\"X\"] use data[[\"X\"]])\n",
    "- Convert the values of X column of all_text into lowercase using \"lower()\" method.\n",
    "\n",
    "<h3>TF-IDF vectoriser</h3>\n",
    "\n",
    "- Initialise a \"TfidfVectorizer()\" object with parameter stop_words= \"english\" and store it in 'tfidf'\n",
    "- Apply the \"fit_transform()\" method of 'tfidf' on X column and store the result in 'vector_tfidf'\n",
    "- Convert 'vector_tfidf' into an array using \"toarray()\" method and store the result in a new variable 'X_tfidf'\n",
    "\n",
    "<h3>For y</h3>\n",
    "\n",
    "- Store the \"y\" column of dataframe 'data'in a new dataframe 'labels'(i.e. instead of data[\"y\"] use data[[\"y\"]])\n",
    "- We need to label encode the values. Therefore initialise a \"LabelEncoder()\" object and store it in 'le'\n",
    "- Use the fit_transform method of 'le' on column \"y\" of 'labels' and store the results back in y column\n",
    "\n",
    "<h3>Model building</h3>\n",
    "\n",
    "- Split 'X_tfidf' and 'labels[\"y\"]' into X_train,X_test,y_train,y_test using train_test_split() function. Use test_size = 0.4 and random_state = 42\n",
    "- Initialise a naive bayes model with MultinomialNB() having and save it to a variable called 'nb'\n",
    "- Fit the model on the training data 'X_train' and 'y_train' using the 'fit()' method.\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'nb_acc'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(path_2)\n",
    "\n",
    "data = data[['Consumer complaint narrative', 'Product']]\n",
    "\n",
    "data.rename(columns={'Product' : 'y', 'Consumer complaint narrative' : 'X'}, inplace=True)\n",
    "\n",
    "data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>when my loan was switched over to navient i wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i tried to sign up for a spending monitoring p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>my mortgage is with bb &amp; t bank, recently i ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>the entire lending experience with citizens ba...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>my credit score has gone down xxxx points in t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    X\n",
       "1   when my loan was switched over to navient i wa...\n",
       "2   i tried to sign up for a spending monitoring p...\n",
       "7   my mortgage is with bb & t bank, recently i ha...\n",
       "13  the entire lending experience with citizens ba...\n",
       "14  my credit score has gone down xxxx points in t..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_text = data[['X']]\n",
    "\n",
    "all_text = all_text.applymap(lambda x : x.lower())\n",
    "all_text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "vector_tfidf = tfidf.fit_transform(all_text['X'])\n",
    "\n",
    "X_tfidf = vector_tfidf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "labels = data[['y']]\n",
    "\n",
    "le = LabelEncoder()\n",
    "\n",
    "labels['y'] = le.fit_transform(labels['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tfidf, labels['y'], test_size=0.4, random_state=42)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train, y_train)\n",
    "\n",
    "nb_acc = nb.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.42461964038727523"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Sampling</h3>\n",
    "\n",
    "- In this task, we will oversample the data and try to implement Naive Bayes classifier again.\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "\n",
    "- Initialise a RandomOverSampler() object with random_state=0 and save it to a variable called 'ros'.\n",
    "- Using fit_sample() method of 'ros', undersample 'X_train' and 'y_train' and store the new samples in variables 'X_ros' and 'y_ros'.\n",
    "- Initialise a naive bayes model and save it to a variable called 'nb'.\n",
    "- Fit the model on the training data 'X_ros' and 'y_ros' using the 'fit()' method.\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'ros_score'\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.6016597510373444"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ros = RandomOverSampler(random_state=0)\n",
    "\n",
    "X_ros, y_ros = ros.fit_sample(X_train, y_train)\n",
    "\n",
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_ros, y_ros)\n",
    "\n",
    "ros_score = nb.score(X_test, y_test)\n",
    "\n",
    "ros_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "<h3>Linear SVC</h3>\n",
    "\n",
    "- Let's use a Linear SVC as our last algorithm, to test the results.\n",
    "- Initialise a support vector model with SVC() with random_state=0& kernel=\"linear\" and save it to a variable called 'svc'.\n",
    "- Fit the model on the training data 'X_ros' and 'y_ros' using the 'fit()' method.\n",
    "- Find out the accuracy score between X_test and 'y_test' using the 'score()' method and save it in a variable called 'svc_score'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = SVC(random_state=0, kernel='linear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='auto_deprecated',\n",
       "    kernel='linear', max_iter=-1, probability=False, random_state=0,\n",
       "    shrinking=True, tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.648686030428769"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svc.fit(X_ros, y_ros)\n",
    "\n",
    "svc_score = svc.score(X_test, y_test)\n",
    "\n",
    "svc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "# Assessment\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First task for you guys is to load data, take only the relevant columns and observing the distribution of the class labels.\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "\n",
    "- Load data using path variable using .read_csv() method of pandas. Save it as news\n",
    "- All the columns are not relevant and you will be going forward with TITLE (title of resource) and CATEGORY (class label). Select only these two features and store it as news\n",
    "- Now, get the class distribution of class lables. You can get it using .value_counts() method on CATEGORY column of news dataframe. Store it inside dist variable\n",
    "- Print out dist and news.head() to observe the class-wise distributions and the first observations respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fed official says weak data caused by weather,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fed's Charles Plosser sees high bar for change...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>US open: Stocks fall after Fed official hints ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fed risks falling 'behind the curve', Charles ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fed's Plosser: Nasty Weather Has Curbed Job Gr...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               TITLE CATEGORY\n",
       "0  Fed official says weak data caused by weather,...        b\n",
       "1  Fed's Charles Plosser sees high bar for change...        b\n",
       "2  US open: Stocks fall after Fed official hints ...        b\n",
       "3  Fed risks falling 'behind the curve', Charles ...        b\n",
       "4  Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "e    152469\n",
       "b    115967\n",
       "t    108344\n",
       "m     45639\n",
       "Name: CATEGORY, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '/Users/raisaurabh04/OneDrive/GreyAtom/NLP_Assessment.zip'\n",
    "\n",
    "news = pd.read_csv(path)\n",
    "\n",
    "news = news[['TITLE', 'CATEGORY']]\n",
    "\n",
    "dist = news['CATEGORY'].value_counts()\n",
    "\n",
    "news.head()\n",
    "\n",
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "\n",
    "- Initialize stopwords as stop with set(stopwords.words('english'))\n",
    "\n",
    "- To retain only alphabets for every instance, use a lambda function in combination with .apply() method that does so. The function that you will be applying to every instance (supposing the instance is row) will be re.sub(\"[^a-zA-Z]\", \" \",x). Remember this operation should be carried out on TITLE column only\n",
    "\n",
    "- Next use lambda function and .apply() method to first convert the instances to lowercase (using .lower()) and then tokenize (using .split()). Remember this operation should be carried out on TITLE column only\n",
    "\n",
    "- Now time to remove stopwords from every instance. Again using a combination of lambda function and .apply() method retain only words which are in that instance but not in stop. You can take the help of a list comprehension to achieve it. For ex: [i for i in x if i not in y]. Remember this operation should be carried out on TITLE column only\n",
    "\n",
    "- The steps mentioned above gives a list for every instance across TITLE column. Join the list elements into a single sentence using ' '.join() method of lists. Use both lambda function and .apply() method for it.\n",
    "\n",
    "- Finally split into train and test using train_test_split function where feature is news[\"TITLE\"], target is news[\"CATEGORY\"], test size is 20% and random state is 3. Save the resultant variables as X_train, X_test, Y_train and Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop = set(stopwords.words('english'))\n",
    "\n",
    "news['TITLE'] = news['TITLE'].map(lambda x : re.sub('[^a-zA-Z]', ' ', x).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['TITLE'] = news['TITLE'].map(lambda x : x.split())        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         [fed, official, says, weak, data, caused, weat...\n",
       "1         [fed, charles, plosser, sees, high, bar, chang...\n",
       "2         [us, open, stocks, fall, fed, official, hints,...\n",
       "3         [fed, risks, falling, behind, curve, charles, ...\n",
       "4         [fed, plosser, nasty, weather, curbed, job, gr...\n",
       "5           [plosser, fed, may, accelerate, tapering, pace]\n",
       "6                    [fed, plosser, taper, pace, may, slow]\n",
       "7         [fed, plosser, expects, us, unemployment, fall...\n",
       "8         [us, jobs, growth, last, month, hit, weather, ...\n",
       "9         [ecb, unlikely, end, sterilisation, smp, purch...\n",
       "10        [ecb, unlikely, end, sterilization, smp, purch...\n",
       "11              [eu, half, baked, bank, union, could, work]\n",
       "12         [europe, reaches, crunch, point, banking, union]\n",
       "13        [ecb, focus, stronger, euro, drowns, ecb, mess...\n",
       "14               [eu, aims, deal, tackling, failing, banks]\n",
       "15            [forex, pound, drops, one, month, lows, euro]\n",
       "16        [noyer, says, strong, euro, creates, unwarrant...\n",
       "17        [eu, week, ahead, march, bank, resolution, tra...\n",
       "18                 [ecb, member, noyer, open, kinds, tools]\n",
       "19        [euro, anxieties, wane, bunds, top, treasuries...\n",
       "20        [noyer, says, strong, euro, creates, unwarrant...\n",
       "21        [noyer, says, stronger, euro, creates, unwarra...\n",
       "22        [bad, loan, triggers, key, feature, ecb, bank,...\n",
       "23        [china, trade, deficit, structural, worries, e...\n",
       "24                            [things, need, know, morning]\n",
       "25              [ecb, noyer, happy, euro, strength, update]\n",
       "26        [eurozone, banks, sovereign, exposure, hits, n...\n",
       "27        [ecb, reveal, bad, loan, hurdles, euro, zone, ...\n",
       "28        [forex, market, eur, usd, retreats, year, high...\n",
       "29        [refile, bad, loan, triggers, key, feature, ec...\n",
       "                                ...                        \n",
       "422389    [cdc, director, says, liberia, ebola, outbreak...\n",
       "422390    [france, asks, citizens, avoid, liberia, sierr...\n",
       "422391    [ebola, infected, doctor, sierra, leone, sahr,...\n",
       "422392    [ebola, zone, nations, isolated, airlines, sto...\n",
       "422393    [ebola, response, dangerously, inadequate, say...\n",
       "422394    [glaxo, ebola, vaccine, may, begin, safety, te...\n",
       "422395    [ebola, causing, huge, damage, west, african, ...\n",
       "422396               [ebola, outbreak, even, worse, feared]\n",
       "422397          [cdc, ebola, outbreak, even, worse, feared]\n",
       "422398    [ebola, virus, disease, democratic, republic, ...\n",
       "422399    [cdc, chief, warns, liberia, ebola, crisis, ge...\n",
       "422400    [update, ebola, causing, huge, damage, w, afri...\n",
       "422401    [removes, staff, sierra, leone, ebola, center,...\n",
       "422402    [ebola, causing, huge, damage, w, africa, econ...\n",
       "422403    [democratic, republic, congo, reports, ebola, ...\n",
       "422404          [british, ebola, sufferer, gets, new, drug]\n",
       "422405    [afdb, boss, says, ebola, infected, countries,...\n",
       "422406    [cincinnati, children, rebuild, boy, throat, u...\n",
       "422407    [cincinnati, doctors, use, child, rib, rebuild...\n",
       "422408    [cincinnati, children, surgeons, rebuild, thro...\n",
       "422409    [surgeons, remove, year, old, rib, rebuild, da...\n",
       "422410           [surgery, update, boy, swallowed, battery]\n",
       "422411           [boy, swallowed, battery, headed, surgery]\n",
       "422412        [ohio, boy, throat, rebuilt, burned, battery]\n",
       "422413    [cincinnati, children, surgeons, rebuild, thro...\n",
       "422414    [surgeons, remove, year, old, rib, rebuild, da...\n",
       "422415    [boy, surgery, esophagus, battery, burns, hole...\n",
       "422416    [child, swallowed, battery, reconstructive, su...\n",
       "422417    [phoenix, boy, undergoes, surgery, repair, thr...\n",
       "422418    [phoenix, boy, undergoes, surgery, repair, thr...\n",
       "Name: TITLE, Length: 422419, dtype: object"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news['TITLE'].apply(lambda x : [i for i in x if i not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "news['TITLE'] = news['TITLE'].apply(lambda x : [i for i in x if i not in stop])\n",
    "\n",
    "news['TITLE'] = news['TITLE'].apply(lambda x : \" \".join(x))        \n",
    "\n",
    "X = news['TITLE']\n",
    "\n",
    "y = news['CATEGORY']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TITLE</th>\n",
       "      <th>CATEGORY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>fed official says weak data caused weather slo...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fed charles plosser sees high bar change pace ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>us open stocks fall fed official hints acceler...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fed risks falling behind curve charles plosser...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fed plosser nasty weather curbed job growth</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>plosser fed may accelerate tapering pace</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fed plosser taper pace may slow</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>fed plosser expects us unemployment fall end</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>us jobs growth last month hit weather fed pres...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ecb unlikely end sterilisation smp purchases t...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ecb unlikely end sterilization smp purchases t...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>eu half baked bank union could work</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>europe reaches crunch point banking union</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>ecb focus stronger euro drowns ecb message kee...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>eu aims deal tackling failing banks</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>forex pound drops one month lows euro</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>noyer says strong euro creates unwarranted eco...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>eu week ahead march bank resolution transparen...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>ecb member noyer open kinds tools</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>euro anxieties wane bunds top treasuries spain...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>noyer says strong euro creates unwarranted eco...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>noyer says stronger euro creates unwarranted p...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>bad loan triggers key feature ecb bank test an...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>china trade deficit structural worries europe ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>things need know morning</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ecb noyer happy euro strength update</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>eurozone banks sovereign exposure hits new high</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>ecb reveal bad loan hurdles euro zone bank test</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>forex market eur usd retreats year highs stron...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>refile bad loan triggers key feature ecb bank ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422389</th>\n",
       "      <td>cdc director says liberia ebola outbreak get w...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422390</th>\n",
       "      <td>france asks citizens avoid liberia sierra leon...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422391</th>\n",
       "      <td>ebola infected doctor sierra leone sahr rogers...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422392</th>\n",
       "      <td>ebola zone nations isolated airlines stop flights</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422393</th>\n",
       "      <td>ebola response dangerously inadequate says msf...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422394</th>\n",
       "      <td>glaxo ebola vaccine may begin safety tests hum...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422395</th>\n",
       "      <td>ebola causing huge damage west african economies</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422396</th>\n",
       "      <td>ebola outbreak even worse feared</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422397</th>\n",
       "      <td>cdc ebola outbreak even worse feared</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422398</th>\n",
       "      <td>ebola virus disease democratic republic congo ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422399</th>\n",
       "      <td>cdc chief warns liberia ebola crisis get worse</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422400</th>\n",
       "      <td>update ebola causing huge damage w africa econ...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422401</th>\n",
       "      <td>removes staff sierra leone ebola center amid i...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422402</th>\n",
       "      <td>ebola causing huge damage w africa economies a...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422403</th>\n",
       "      <td>democratic republic congo reports ebola outbreak</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422404</th>\n",
       "      <td>british ebola sufferer gets new drug</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422405</th>\n",
       "      <td>afdb boss says ebola infected countries may lo...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422406</th>\n",
       "      <td>cincinnati children rebuild boy throat using r...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422407</th>\n",
       "      <td>cincinnati doctors use child rib rebuild throat</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422408</th>\n",
       "      <td>cincinnati children surgeons rebuild throat ye...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422409</th>\n",
       "      <td>surgeons remove year old rib rebuild damaged t...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422410</th>\n",
       "      <td>surgery update boy swallowed battery</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422411</th>\n",
       "      <td>boy swallowed battery headed surgery</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422412</th>\n",
       "      <td>ohio boy throat rebuilt burned battery</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422413</th>\n",
       "      <td>cincinnati children surgeons rebuild throat ye...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422414</th>\n",
       "      <td>surgeons remove year old rib rebuild damaged t...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422415</th>\n",
       "      <td>boy surgery esophagus battery burns hole throat</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422416</th>\n",
       "      <td>child swallowed battery reconstructive surgery...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422417</th>\n",
       "      <td>phoenix boy undergoes surgery repair throat da...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>422418</th>\n",
       "      <td>phoenix boy undergoes surgery repair throat da...</td>\n",
       "      <td>m</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>422419 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    TITLE CATEGORY\n",
       "0       fed official says weak data caused weather slo...        b\n",
       "1       fed charles plosser sees high bar change pace ...        b\n",
       "2       us open stocks fall fed official hints acceler...        b\n",
       "3       fed risks falling behind curve charles plosser...        b\n",
       "4             fed plosser nasty weather curbed job growth        b\n",
       "5                plosser fed may accelerate tapering pace        b\n",
       "6                         fed plosser taper pace may slow        b\n",
       "7            fed plosser expects us unemployment fall end        b\n",
       "8       us jobs growth last month hit weather fed pres...        b\n",
       "9       ecb unlikely end sterilisation smp purchases t...        b\n",
       "10      ecb unlikely end sterilization smp purchases t...        b\n",
       "11                    eu half baked bank union could work        b\n",
       "12              europe reaches crunch point banking union        b\n",
       "13      ecb focus stronger euro drowns ecb message kee...        b\n",
       "14                    eu aims deal tackling failing banks        b\n",
       "15                  forex pound drops one month lows euro        b\n",
       "16      noyer says strong euro creates unwarranted eco...        b\n",
       "17      eu week ahead march bank resolution transparen...        b\n",
       "18                      ecb member noyer open kinds tools        b\n",
       "19      euro anxieties wane bunds top treasuries spain...        b\n",
       "20      noyer says strong euro creates unwarranted eco...        b\n",
       "21      noyer says stronger euro creates unwarranted p...        b\n",
       "22      bad loan triggers key feature ecb bank test an...        b\n",
       "23      china trade deficit structural worries europe ...        b\n",
       "24                               things need know morning        b\n",
       "25                   ecb noyer happy euro strength update        b\n",
       "26        eurozone banks sovereign exposure hits new high        b\n",
       "27        ecb reveal bad loan hurdles euro zone bank test        b\n",
       "28      forex market eur usd retreats year highs stron...        b\n",
       "29      refile bad loan triggers key feature ecb bank ...        b\n",
       "...                                                   ...      ...\n",
       "422389  cdc director says liberia ebola outbreak get w...        m\n",
       "422390  france asks citizens avoid liberia sierra leon...        m\n",
       "422391  ebola infected doctor sierra leone sahr rogers...        m\n",
       "422392  ebola zone nations isolated airlines stop flights        m\n",
       "422393  ebola response dangerously inadequate says msf...        m\n",
       "422394  glaxo ebola vaccine may begin safety tests hum...        m\n",
       "422395   ebola causing huge damage west african economies        m\n",
       "422396                   ebola outbreak even worse feared        m\n",
       "422397               cdc ebola outbreak even worse feared        m\n",
       "422398  ebola virus disease democratic republic congo ...        m\n",
       "422399     cdc chief warns liberia ebola crisis get worse        m\n",
       "422400  update ebola causing huge damage w africa econ...        m\n",
       "422401  removes staff sierra leone ebola center amid i...        m\n",
       "422402  ebola causing huge damage w africa economies a...        m\n",
       "422403   democratic republic congo reports ebola outbreak        m\n",
       "422404               british ebola sufferer gets new drug        m\n",
       "422405  afdb boss says ebola infected countries may lo...        m\n",
       "422406  cincinnati children rebuild boy throat using r...        m\n",
       "422407    cincinnati doctors use child rib rebuild throat        m\n",
       "422408  cincinnati children surgeons rebuild throat ye...        m\n",
       "422409  surgeons remove year old rib rebuild damaged t...        m\n",
       "422410               surgery update boy swallowed battery        m\n",
       "422411               boy swallowed battery headed surgery        m\n",
       "422412             ohio boy throat rebuilt burned battery        m\n",
       "422413  cincinnati children surgeons rebuild throat ye...        m\n",
       "422414  surgeons remove year old rib rebuild damaged t...        m\n",
       "422415    boy surgery esophagus battery burns hole throat        m\n",
       "422416  child swallowed battery reconstructive surgery...        m\n",
       "422417  phoenix boy undergoes surgery repair throat da...        m\n",
       "422418  phoenix boy undergoes surgery repair throat da...        m\n",
       "\n",
       "[422419 rows x 2 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "\n",
    "- Initialize Bag-of-words vectorizer using CountVectorizer() and TF-IDF vectorizer using TfidfVectorizer(ngram_range=(1,3)). Save them as count_vectorizer and tfidf_vectorizer respectively\n",
    "\n",
    "- Next thing to do is fit each vectorizer on training and test features with text data and transform them to vectors.\n",
    "\n",
    "    * First fit and transform data with count_vectorizer on X_train using .fit_transform(X_train) method of count_vectorizer and save it as X_train_count\n",
    "\n",
    "    * Use this fitted version of count_vectorizer on X_test and transform X_test with .transform(X_test) method of count_vectorizer. Save it as X_test_count\n",
    "\n",
    "    * Similarly repeat the previous two steps with tfidf_vectorizer and save the transformed training feature as X_train_tfidf and transformed test feature as X_test_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer()\n",
    "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "X_train_count = count_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_count = count_vectorizer.transform(X_test)\n",
    "\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Instructions</h3>\n",
    "\n",
    "- First initialize two Multinomial Naive Bayes classifiers with MultinomialNB() and save them as nb_1 and nb_2. The reason for initializing two classifiers is because you will be training and testing on both Bag-of-words and TF-IDF transformed training data\n",
    "\n",
    "- Fit nb_1 on X_train_count and Y_train using .fit() method\n",
    "\n",
    "- Fit nb_2 on X_train_tfidf and Y_train using .fit() method\n",
    "\n",
    "- Find the accuracy with Bag-of-words approach using accuracy_score(nb_1.predict(X_test_count), Y_test) and save it as acc_count_nb\n",
    "\n",
    "- Similarly find the accuracy for the TF-IDF approach (only difference is the classifer is nb_2) and save it as acc_tfidf_nb\n",
    "\n",
    "- Print out acc_count_nb and acc_tfidf_nb to check which version performs better for with Multinomial Naive Bayes as classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9268618910089484"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9323895648880262"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_1, nb_2 = MultinomialNB(), MultinomialNB()\n",
    "\n",
    "nb_1.fit(X_train_count, y_train)\n",
    "\n",
    "nb_2.fit(X_train_tfidf, y_train)\n",
    "\n",
    "acc_count_nb = accuracy_score(y_test, nb_1.predict(X_test_count))\n",
    "\n",
    "acc_count_nb\n",
    "\n",
    "acc_tfidf_nb = accuracy_score(y_test, nb_2.predict(X_test_tfidf))\n",
    "\n",
    "acc_tfidf_nb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<h3>Instructions</h3>\n",
    "\n",
    "- First initialize two classifiers with OneVsRestClassifier(LogisticRegression(random_state=10)) and save them as logreg_1 and logreg_2. The reason for initializing two classifiers is because you will be training and testing on both Bag-of-words and TF-IDF transformed training data\n",
    "- Fit logreg_1 on X_train_count and Y_train using .fit() method\n",
    "- Fit logreg_2 on X_train_tfidf and Y_train using .fit() method\n",
    "- Find the accuracy with Bag-of-words approach using accuracy_score(logreg_1.predict(X_test_count), Y_test) and save it as acc_count_logreg\n",
    "- Similarly find the accuracy for the TF-IDF approach (only difference is the classifer is logreg_2) and save it as acc_tfidf_logreg\n",
    "- Print out acc_count_logreg and acc_tfidf_logreg to check which version performs better for with Multinomial Naive Bayes as classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=10, solver='warn',\n",
       "                                                 tol=0.0001, verbose=0,\n",
       "                                                 warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raisaurabh04/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OneVsRestClassifier(estimator=LogisticRegression(C=1.0, class_weight=None,\n",
       "                                                 dual=False, fit_intercept=True,\n",
       "                                                 intercept_scaling=1,\n",
       "                                                 l1_ratio=None, max_iter=100,\n",
       "                                                 multi_class='warn',\n",
       "                                                 n_jobs=None, penalty='l2',\n",
       "                                                 random_state=10, solver='warn',\n",
       "                                                 tol=0.0001, verbose=0,\n",
       "                                                 warm_start=False),\n",
       "                    n_jobs=None)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9464632356422518"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0.9428649211685053"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg_1, logreg_2 = OneVsRestClassifier(LogisticRegression(random_state=10)), OneVsRestClassifier(LogisticRegression(random_state=10))\n",
    "\n",
    "logreg_1.fit(X_train_count, y_train)\n",
    "\n",
    "logreg_2.fit(X_train_tfidf, y_train)\n",
    "\n",
    "acc_count_logreg = accuracy_score(y_test, logreg_1.predict(X_test_count))\n",
    "\n",
    "acc_count_logreg\n",
    "\n",
    "acc_tfidf_logreg = accuracy_score(y_test, logreg_2.predict(X_test_tfidf))\n",
    "\n",
    "acc_tfidf_logreg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
