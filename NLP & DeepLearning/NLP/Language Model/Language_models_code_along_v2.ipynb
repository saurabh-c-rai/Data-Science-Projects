{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YInsm7I_lnkL"
   },
   "source": [
    "# Find the bigram probabilities of the sentence tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": "[nltk_data] Downloading package brown to\n[nltk_data]     /Users/raisaurabh04/nltk_data...\n[nltk_data]   Package brown is already up-to-date!\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', ...]"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "# Corpus\n",
    "words = brown.words()\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "['the',\n 'fulton',\n 'county',\n 'grand',\n 'jury',\n 'said',\n 'friday',\n 'an',\n 'investigation',\n 'of']"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "words=[w.lower() for w in words]\n",
    "\n",
    "words[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1161192"
     },
     "metadata": {},
     "execution_count": 80
    }
   ],
   "source": [
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FreqDist({'the': 69971, ',': 58334, '.': 49346, 'of': 36412, 'and': 28853, 'to': 26158, 'a': 23195, 'in': 21337, 'that': 10594, 'is': 10109, ...})"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "uni_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1161192"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "total_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Frequency of tokens of the sample sentence: 1161192\n"
    }
   ],
   "source": [
    "print('Frequency of tokens of the sample sentence:',total_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentence \n",
    "test_sentence_tokens=['this','is','a','sunny','day','.','however','i','am','not','feeling','well','lots','of','cold']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Frequency of \"this\" is 5145\nFrequency of \"is\" is 10109\nFrequency of \"a\" is 23195\nFrequency of \"sunny\" is 13\nFrequency of \"day\" is 687\nFrequency of \".\" is 49346\nFrequency of \"however\" is 552\nFrequency of \"i\" is 5164\nFrequency of \"am\" is 237\nFrequency of \"not\" is 4610\nFrequency of \"feeling\" is 172\nFrequency of \"well\" is 897\nFrequency of \"lots\" is 42\nFrequency of \"of\" is 36412\nFrequency of \"cold\" is 171\n\n\n\n"
    }
   ],
   "source": [
    "for word in test_sentence_tokens:\n",
    "    print(f'Frequency of \\\"{word}\\\" is {uni_freq[word]}')\n",
    "print('\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    #print(f\"Word is {word}\")\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "        bigram_words.append(word)\n",
    "        #print(f\"Previous, word in if block is {previous}, {word}\")\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "        #print(f\"Previous, word in else block is {previous}, {word}\")\n",
    "    previous = word\n",
    "    #print(bigram_words)\n",
    "\n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*\n"
    }
   ],
   "source": [
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "69971"
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "freq = Counter(bigram_words)\n",
    "freq['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "FreqDist({'the': 69971, ',': 58334, '*start_end*': 55636, '.': 49346, 'of': 36412, 'and': 28853, 'to': 26158, 'a': 23195, 'in': 21337, 'that': 10594, ...})"
     },
     "metadata": {},
     "execution_count": 62
    }
   ],
   "source": [
    "updated_uni_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "49346"
     },
     "metadata": {},
     "execution_count": 63
    }
   ],
   "source": [
    "updated_uni_freq[\".\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<ConditionalFreqDist with 49816 conditions>"
     },
     "metadata": {},
     "execution_count": 65
    }
   ],
   "source": [
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "conditional_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "0.08576515975863093"
     },
     "metadata": {},
     "execution_count": 86
    }
   ],
   "source": [
    "conditional_freq['is']['a'] / updated_uni_freq['is']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "5145"
     },
     "metadata": {},
     "execution_count": 81
    }
   ],
   "source": [
    "updated_uni_freq['this']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "    \n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "\n",
    "    #print(f\"First Token is \\\"{first}\\\", previous token is \\\"{second}\\\",unigram          freq. is {unigram_freq}\")\n",
    "    bigram_prob = (bigram_freq)/(unigram_freq)\n",
    "    return bigram_prob\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "*start_end* this 0.0196\nthis is 0.0842\nis a 0.0858\na sunny 4.31e-05\nsunny day 0.154\nday . 0.162\n. however 0.0\nhowever i 0.0\ni am 0.0401\nam not 0.105\nnot feeling 0.0\nfeeling well 0.0\nwell lots 0.0\nlots of 0.714\nof cold 0.000137\n"
    }
   ],
   "source": [
    "## Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "  \n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*\n*start_end* this 0.0196\nthis is 0.0842\nis a 0.0858\na sunny 4.31e-05\nsunny day 0.154\nday . 0.0\n"
    },
    {
     "output_type": "error",
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-22d8d319f395>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_sentence_tokens\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mnext_probability\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_bigram_probability\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprevious\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%.3g'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mnext_probability\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mprevious\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-29-22d8d319f395>\u001b[0m in \u001b[0;36mget_bigram_probability\u001b[0;34m(first, second)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0munigram_freq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdated_uni_freq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfirst\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0mbigram_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbigram_freq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munigram_freq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#without Laplacian smoothing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(previous)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('Calculating bigram probalities for sentence, including bigrams with sentence boundaries, i.e., *start_end*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "    \n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "\n",
    "    bigram_prob = (bigram_freq)/(unigram_freq) #without Laplacian smoothing\n",
    "\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "## Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "phWw7hty3PXm"
   },
   "source": [
    "##Find the perplexity and total probabilities of the given sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YckL36_wQ_nJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_nsIGYlQ_hq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnpHhJteQ_W-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "LbmJnULf212C",
    "outputId": "54f7d1ef-f5bb-4a3e-d183-4620dab11a51"
   },
   "outputs": [],
   "source": [
    "prob_list=[0.1, 0.023 ,0.09]\n",
    "\n",
    "\n",
    "perplexity=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list)-1\n",
    "\n",
    "\n",
    "# Calculating the perplexity\n",
    "for val in prob_list:\n",
    "    perplexity = perplexity * (1/val)\n",
    "\n",
    "perplexity = pow(perplexity, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity= :\",perplexity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "For the sentence- 'this is a sunny day'\nTotal probability: 2.494655687321879e-10\nPerplexity: 251.6212681454414\n\n\nFor the sentence- 'this place is beautiful'\nTotal probability:  4.009684736463708e-11\nPerplexity:  2921.6616783932823\n"
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\"For the sentence: 'this is a sunny day' \"\"\" \n",
    "prob_list_1=[0.008303975842979365, 0.05030826140567201, 0.08609535184632229, 4.5083630133898384e-05, 0.15384615384615385]\n",
    "\n",
    "\n",
    "\n",
    "total_prob_1 = 1\n",
    "\n",
    "# Multiplying all the values of the probability and storing it\n",
    "for val in prob_list_1:\n",
    "    total_prob_1 *= val\n",
    "\n",
    "\n",
    "print(\"For the sentence- 'this is a sunny day'\")\n",
    "print(\"Total probability:\",total_prob_1)\n",
    "\n",
    "\n",
    "perplexity_1=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list_1)-1\n",
    "\n",
    "\n",
    "# Calculating the perplexity\n",
    "for val in prob_list_1:\n",
    "    perplexity_1 = perplexity_1 * (1/val)\n",
    "\n",
    "perplexity_1 = pow(perplexity_1, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity:\",perplexity_1)\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"For the sentence: 'this place is beautiful' \"\"\"\n",
    "prob_list_2=[0.008303975842979365, 0.0022194821208384712, 0.02185792349726776, 9.953219866626854e-05]\n",
    "\n",
    "total_prob_2 = 1\n",
    "\n",
    "# Multiplying all the values of the probability and storing it\n",
    "for val in prob_list_2:\n",
    "    total_prob_2 *= val\n",
    "\n",
    "print(\"\\n\\nFor the sentence- 'this place is beautiful'\")    \n",
    "print(\"Total probability: \",total_prob_2)\n",
    "\n",
    "\n",
    "perplexity_2=1\n",
    "\n",
    "# Calculating N\n",
    "N=len(prob_list_2)-1\n",
    "\n",
    "# Calculating perplexity\n",
    "for val in prob_list_2:\n",
    "    perplexity_2 = perplexity_2 * (1/val)\n",
    "\n",
    "perplexity_2 = pow(perplexity_2, 1/float(N)) \n",
    "\n",
    "print(\"Perplexity: \",perplexity_2)\n",
    "\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VZZX7KkDSF0Z"
   },
   "outputs": [],
   "source": [
    "# Naive Bayes Model:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z8NpG8apSFke"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OFbNJURr4dYV"
   },
   "source": [
    "##Calculate the probability using Laplace smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 272
    },
    "colab_type": "code",
    "id": "LhlEVNGq3hxa",
    "outputId": "7f3bdf42-599a-44c2-a3dd-81a0d9a9b6f2"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "# Corpus\n",
    "words = brown.words()\n",
    "words=[w.lower() for w in words]\n",
    "\n",
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens of the sample sentence:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,uni_freq[word])\n",
    "\n",
    "    \n",
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('\\nCalculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "#Sentence \n",
    "test_sentence_tokens=['sunset','looks','magnificient','.']\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "\n",
    "V=len(set(words))\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "    \n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "\n",
    "    bigram_prob = (bigram_freq + 1)/(unigram_freq + V) # with Laplacian Smoothing\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "# Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    "\n",
    "\n",
    "\n",
    "# Calculating the total probability\n",
    "\n",
    "total_prob = 1\n",
    "for val in prob_list:\n",
    "    total_prob *= val\n",
    "\n",
    "print(\"\\nTotal probability:\",total_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k7nJg8bI5fJ2"
   },
   "source": [
    "##Calculate the probability using Backoff method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "_9bP7Ub85cQk",
    "outputId": "2ef332ea-1139-42f5-99cc-8f2e9b68238a"
   },
   "outputs": [],
   "source": [
    "\n",
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "\n",
    "#Sentence \n",
    "test_sentence_tokens=['this','is','a','very','sunny','day','.']\n",
    "\n",
    "\n",
    "# Corpus\n",
    "words = brown.words()\n",
    "words=[w.lower() for w in words]\n",
    "\n",
    "# Unigram frequency \n",
    "uni_freq = nltk.FreqDist(w.lower() for w in words)\n",
    "\n",
    "# Size of corpus\n",
    "total_words = len(words)\n",
    "\n",
    "print('Frequency of tokens of the sample sentence:')\n",
    "\n",
    "for word in test_sentence_tokens:\n",
    "    print(word,uni_freq[word])\n",
    "\n",
    "    \n",
    "# Creating bigrams\n",
    "\n",
    "bigram_words = []\n",
    "previous = 'EMPTY'\n",
    "sentences = 0\n",
    "for word in words:\n",
    "    if previous in ['EMPTY','.','?','!']:\n",
    "        ## insert word_boundaries at beginning of Brown,\n",
    "        bigram_words.append('*start_end*')\n",
    "    else:\n",
    "        bigram_words.append(word)\n",
    "    \n",
    "    previous = word\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "bigram_words.append('*start_end*') ## assume one additional *start_end* at the end of Brown\n",
    "\n",
    "updated_uni_freq  = nltk.FreqDist(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "print('\\nCalculating bigram counts for sentence, including bigrams with sentence boundaries, i.e., *BEGIN* and *END*')\n",
    "\n",
    "\n",
    "# Bigram corpus\n",
    "bigrams = nltk.bigrams(w.lower() for w in bigram_words)\n",
    "\n",
    "\n",
    "# Bigram probabilities\n",
    "conditional_freq = nltk.ConditionalFreqDist(bigrams)\n",
    "\n",
    "\n",
    "# Code begins here\n",
    "\n",
    "\n",
    "V=len(set(words))\n",
    "\n",
    "\n",
    "# Function to calculate bigram probability\n",
    "def get_bigram_probability(first,second):\n",
    "\n",
    "    if not second in conditional_freq[first]:\n",
    "        print('Backing Off to Unigram Probability for',second)\n",
    "        unigram_prob = updated_uni_freq[second]/len(words)\n",
    "        return unigram_prob \n",
    "    \n",
    "\n",
    "    bigram_freq = conditional_freq[first][second]\n",
    "    unigram_freq = updated_uni_freq[first]\n",
    "    bigram_prob = bigram_freq/unigram_freq\n",
    "    \n",
    "    return bigram_prob\n",
    "\n",
    "\n",
    "# Calculating the bigram probability\n",
    "\n",
    "prob_list=[]\n",
    "previous = '*start_end*'\n",
    "for token in test_sentence_tokens:\n",
    "    next_probability = get_bigram_probability(previous,token)\n",
    "    print(previous,token,(float('%.3g' % next_probability)))\n",
    "    previous = token\n",
    "    prob_list.append(next_probability)\n",
    "\n",
    "    \n",
    "# For the final term    \n",
    "next_probability = get_bigram_probability(previous,'*start_end*')\n",
    "print(previous,'*start_end*',next_probability)\n",
    "prob_list.append(next_probability)    \n",
    "\n",
    "print(prob_list)    \n",
    "\n",
    "\n",
    "\n",
    "# Calculating the total probability\n",
    "\n",
    "total_prob = 1\n",
    "for val in prob_list:\n",
    "    total_prob *= val\n",
    "\n",
    "print(\"\\nTotal probability:\",total_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wgavtcD25vdc"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Language_models_code_along.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}