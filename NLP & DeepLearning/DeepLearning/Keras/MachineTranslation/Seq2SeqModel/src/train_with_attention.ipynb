{"cells":[{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","collapsed":true,"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":false},"cell_type":"markdown","source":["We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n","\n","There are a variety of languages available, but we'll use the English-French dataset like in the previous chapter. Here is a breif description of the process that we are going to follow to prepare the dataset.\n","\n","1. Add a *start* and *end* token to each sentence.\n","\n","2. Clean the sentences by removing special characters.\n","\n","3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).\n","\n","4. Pad each sentence to a maximum length.\n","\n","As we saw in the last tutorial let's load the dataset for processing"]},{"metadata":{"trusted":true},"cell_type":"code","source":["import pandas as pd\n","import tensorflow as tf\n","import unicodedata\n","import re\n","import numpy as np\n","from sklearn.model_selection import train_test_split\n","import time\n","import matplotlib.pyplot as plt"],"execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["path_to_file=\"/kaggle/input/frenchenglish-translation/fra.txt\"\n","df = pd.read_csv(path_to_file,delimiter='\\t')\n","df.head(5)"],"execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"    Go.      Va !  \\\n0   Hi.   Salut !   \n1   Hi.    Salut.   \n2  Run!   Cours !   \n3  Run!  Courez !   \n4  Who?     Qui ?   \n\n  CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)  \n0  CC-BY 2.0 (France) Attribution: tatoeba.org #5...                               \n1  CC-BY 2.0 (France) Attribution: tatoeba.org #5...                               \n2  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                               \n3  CC-BY 2.0 (France) Attribution: tatoeba.org #9...                               \n4  CC-BY 2.0 (France) Attribution: tatoeba.org #2...                               ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Go.</th>\n      <th>Va !</th>\n      <th>CC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) &amp; #1158250 (Wittydev)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi.</td>\n      <td>Salut !</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Hi.</td>\n      <td>Salut.</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Run!</td>\n      <td>Cours !</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Run!</td>\n      <td>Courez !</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Who?</td>\n      <td>Qui ?</td>\n      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #2...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":["# We will now preprocess the dataset to be used to Model Consumption.\n","\n","## The following class will create a dictionary of words for the dataset. \n","## The dictionary will be in the form of ID-> WORD structure. Forexample, \"mom\"->7\n","\n","class LanguageIndex():\n","    def __init__(self, lang):\n","        self.lang = lang\n","        self.word2idx = {}\n","        self.idx2word = {}\n","        self.vocab = set()\n","\n","        self.create_index()\n","    \n","    def create_index(self):\n","        for phrase in self.lang:\n","            self.vocab.update(phrase.split(' '))\n","\n","        self.vocab = sorted(self.vocab)\n","\n","        self.word2idx['<pad>'] = 0\n","        for index, word in enumerate(self.vocab):\n","            self.word2idx[word] = index + 1\n","\n","        for word, index in self.word2idx.items():\n","            self.idx2word[index] = word"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## Load the dataset in proper format\n","\n","def max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path, num_examples):\n","    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n","    \n","    word_pairs = [[preprocess_sentence(w) for w in l.split('\\t')]  for l in lines[:num_examples]]\n","    \n","    return word_pairs"],"execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def load_dataset(path, num_examples):\n","    # creating cleaned input, output pairs\n","    pairs = create_dataset(path, num_examples)\n","    print(pairs[ : 10])\n","\n","    # index language using the class defined above    \n","    inp_lang = LanguageIndex(sp for en, sp in pairs)\n","    targ_lang = LanguageIndex(en for en, sp in pairs)\n","    \n","    # Vectorize the input and target languages\n","    \n","    # French sentences\n","    input_tensor = [[inp_lang.word2idx[s] for s in sp.split(' ')] for en, sp in pairs]\n","    \n","    # English sentences\n","    target_tensor = [[targ_lang.word2idx[s] for s in en.split(' ')] for en, sp in pairs]\n","    \n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    max_length_inp, max_length_tar = max_length(input_tensor), max_length(target_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor = tf.keras.preprocessing.sequence.pad_sequences(input_tensor, \n","                                                                 maxlen=max_length_inp,\n","                                                                 padding='post')\n","    \n","    target_tensor = tf.keras.preprocessing.sequence.pad_sequences(target_tensor, \n","                                                                  maxlen=max_length_tar, \n","                                                                  padding='post')\n","    \n","    return input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_tar"],"execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def unicode_to_ascii(s):\n","    return ''.join(c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')"],"execution_count":16,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def preprocess_sentence(w):\n","    w = unicode_to_ascii(w.lower().strip())\n","    \n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\" \n","    # Reference:- https://stackoverflow.com/questions/3645931/python-padding-punctuation-with-white-spaces-keeping-punctuation\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    \n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    \n","    w = w.rstrip().strip()\n","    \n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":17,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Try experimenting with the size of that dataset\n","num_examples = 30000\n","input_tensor, target_tensor, inp_lang, targ_lang, max_length_inp, max_length_targ = load_dataset(path_to_file, num_examples)\n","\n","# Creating training and validation sets using an 80-20 split\n","input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n","\n","# Show length\n","len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val)"],"execution_count":18,"outputs":[{"output_type":"stream","text":"[['<start> go . <end>', '<start> va ! <end>', '<start> cc by . france attribution tatoeba . org cm wittydev <end>'], ['<start> hi . <end>', '<start> salut ! <end>', '<start> cc by . france attribution tatoeba . org cm aiji <end>'], ['<start> hi . <end>', '<start> salut . <end>', '<start> cc by . france attribution tatoeba . org cm gillux <end>'], ['<start> run ! <end>', '<start> cours ! <end>', '<start> cc by . france attribution tatoeba . org papabear sacredceltic <end>'], ['<start> run ! <end>', '<start> courez ! <end>', '<start> cc by . france attribution tatoeba . org papabear sacredceltic <end>'], ['<start> who ? <end>', '<start> qui ? <end>', '<start> cc by . france attribution tatoeba . org ck gillux <end>'], ['<start> wow ! <end>', '<start> ca alors ! <end>', '<start> cc by . france attribution tatoeba . org zifre zmoo <end>'], ['<start> fire ! <end>', '<start> au feu ! <end>', '<start> cc by . france attribution tatoeba . org spamster sacredceltic <end>'], ['<start> help ! <end>', '<start> a l aide ! <end>', '<start> cc by . france attribution tatoeba . org lukaszpp sysko <end>'], ['<start> jump . <end>', '<start> saute . <end>', '<start> cc by . france attribution tatoeba . org shishir micsmithel <end>']]\n","name":"stdout"},{"output_type":"error","ename":"ValueError","evalue":"too many values to unpack (expected 2)","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-92b02c9b74e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Try experimenting with the size of that dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minp_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarg_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_inp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length_targ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath_to_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Creating training and validation sets using an 80-20 split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-6cae3ae90a26>\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, num_examples)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# index language using the class defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minp_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtarg_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-4a9091b454d9>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-4a9091b454d9>\u001b[0m in \u001b[0;36mcreate_index\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mphrase\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-15-6cae3ae90a26>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# index language using the class defined above\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0minp_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mtarg_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLanguageIndex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0men\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpairs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"]}]},{"metadata":{"trusted":true},"cell_type":"code","source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","N_BATCH = BUFFER_SIZE//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word2idx)\n","vocab_tar_size = len(targ_lang.word2idx)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let us now write our Encoder Decoder model. Also for this chapter we will be using GRU instead of LSTM for simplicity since GRU has just one state."]},{"metadata":{"trusted":true},"cell_type":"code","source":["# Let up define GRU units for calculation\n","def gru(units):\n","  # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","  # the code automatically does that.\n","    return tf.keras.layers.GRU(units, \n","                               return_sequences=True, \n","                               return_state=True, \n","                               recurrent_activation='sigmoid', \n","                               recurrent_initializer='glorot_uniform')"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Encoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.enc_units)\n","        \n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)        \n","        return output, state\n","    \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))\n","    "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["class Decoder(tf.keras.Model):\n","    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz):\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","        # used for attention\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, hidden, enc_output):\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        \n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis because we are applying tanh(FC(EO) + FC(H)) to self.V\n","        score = self.V(tf.nn.tanh(self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis=1)\n","        \n","        # x shape after passing through embedding == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        \n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","        \n","        return x, state, attention_weights\n","        \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# We will define a loss function to train our Encoder\n","optimizer = tf.train.AdamOptimizer()"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def loss_function(real, pred):\n","    mask = 1 - np.equal(real, 0)\n","    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n","    return tf.reduce_mean(loss_)\n","\n"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["## NOTE: The next cell will take a lot of time so update your epochs accordingly\n","from __future__ import absolute_import, division, print_function"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["EPOCHS = 5\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","    \n","    hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","    \n","    for (batch, (inp, targ)) in enumerate(dataset):\n","        loss = 0\n","        \n","        with tf.GradientTape() as tape:\n","            enc_output, enc_hidden = encoder(inp, hidden)\n","            \n","            dec_hidden = enc_hidden\n","            \n","            dec_input = tf.expand_dims([targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)       \n","            \n","            # Teacher forcing - feeding the target as the next input\n","            for t in range(1, targ.shape[1]):\n","                # passing enc_output to the decoder\n","                predictions, dec_hidden, _ = decoder(dec_input, dec_hidden, enc_output)\n","                \n","                loss += loss_function(targ[:, t], predictions)\n","                \n","                # using teacher forcing\n","                dec_input = tf.expand_dims(targ[:, t], 1)\n","        \n","        batch_loss = (loss / int(targ.shape[1]))\n","        \n","        total_loss += batch_loss\n","        \n","        variables = encoder.variables + decoder.variables\n","        \n","        gradients = tape.gradient(loss, variables)\n","        \n","        optimizer.apply_gradients(zip(gradients, variables))\n","        \n","        if batch % 100 == 0:\n","            print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n","                                                         batch,\n","                                                         batch_loss.numpy()))\n","    # saving (checkpoint) the model every 2 epochs\n","    # if (epoch + 1) % 2 == 0:\n","    #     checkpoint.save(file_prefix = checkpoint_prefix)\n","    \n","    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n","                                        total_loss / N_BATCH))\n","    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"],"execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":["Let's define functions to help us evaluate and translate our model"]},{"metadata":{"trusted":true},"cell_type":"code","source":["def evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    \n","    sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences([inputs], maxlen=max_length_inp, padding='post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(dec_input, dec_hidden, enc_out)\n","        \n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.idx2word[predicted_id] + ' '\n","\n","        if targ_lang.idx2word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","        \n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, sentence, attention_plot"],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize=(10,10))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap='viridis')\n","    \n","    fontdict = {'fontsize': 14}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","    plt.show()\n","    \n","    "],"execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":["def translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ):\n","    result, sentence, attention_plot = evaluate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)\n","        \n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(result))\n","    \n","    attention_plot = attention_plot[:len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))\n","\n","Let's do translation of sentences now\n","\n","sentence=\"Je suis libre\"\n","translate(sentence, encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}